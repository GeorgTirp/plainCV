seed: 0
print_progress: false

dataset: "tiny_imagenet"   # or "tiny_imagenet"
batch_size: 64
image_size: 64             # TinyImageNet uses 64; override when switching datasets
num_epochs: 100
num_channels: 3            # TinyImageNet uses 3 channels
num_classes: 200            # TinyImageNet uses 200 classes

#dataset: "fashion_mnist"   # or "tiny_imagenet"
#batch_size: 64
#image_size: 28            
#num_epochs: 10
#num_channels: 1            
#num_classes: 10           

model: "resnet18"  # or "mlp", "vit_small", "small_resnet", "resnet18"

# Vision Transformer defaults (only used if model starts with "vit")
vit_patch_size: 4
vit_hidden_size: 128
vit_mlp_dim: 256
vit_layers: 4
vit_heads: 4
vit_dropout: 0.1
vit_use_layernorm: false
vit_use_batchnorm: false

# ResNet defaults
resnet_use_batchnorm: true

optim: pns_eigenadam  # or "muon", "adam", "soap", "pns_eigenadam", "hessian_free", "sophia", "sophia_shampoo"
lr: 0.001
weight_decay: 0.01
beta1: 0.9
beta2: 0.9

# Optional Muon-specific fields (all have defaults):
muon_beta: 0.95
muon_ns_steps: 5
muon_ns_coeffs: [3.4445, -4.7750, 2.0315]
muon_nesterov: true
muon_adaptive: false


hf_cg_max_iters: 15   
hf_cg_tol: 1e-2      # looser tolerance
hf_damping: 1e-2

# --- PN-S EigenAdamâ€“specific hyperparameters ---
#pns_sketch_dim: 128
pns_curvature_update_every: 58  
curvature_eigenvectors: 6         
curvature_iters: 6
gradient_eigenvectors: 6              
gradient_iters: 6
pns_precond_damping: 1.0e-4
pns_sqrt_scaling: false       
pns_curvature_backend: ggn  # or "hessian", "ggn", "fisher"
pns_log_curvature: true
pns_split_spaces: false
pns_lr_top: 1e-3
pns_lr_perp: 5e-3

# --- Sophia-specific hyperparameters ---
rho: 0.01
clip_threshold: 1
hessian_update_every: 10
hutchinson_samples: 1

schedule_free: False
schedule_free_lr: 0.01
schedule_free_b1: 0.9
schedule_free_weight_lr_power: 2.0
