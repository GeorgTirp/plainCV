seed: 0

dataset: "tiny_imagenet"   # or "tiny_imagenet"
batch_size: 64
image_size: 64             # TinyImageNet uses 64; override when switching datasets
num_epochs: 10
num_channels: 3            # TinyImageNet uses 3 channels
num_classes: 200            # TinyImageNet uses 200 classes

#dataset: "fashion_mnist"   # or "tiny_imagenet"
#batch_size: 64
#image_size: 28            
#num_epochs: 10
#num_channels: 1            
#num_classes: 10           

model: "vit_small"  # or "mlp", "vit_small", "small_resnet", "resnet18"

# Vision Transformer defaults (only used if model starts with "vit")
vit_patch_size: 4
vit_hidden_size: 128
vit_mlp_dim: 256
vit_layers: 4
vit_heads: 4
vit_dropout: 0.1

optim: muon  # or "muon", "adam", "soap", "pns_eigenadam", "hessian_free", 
lr: 0.001
weight_decay: 0.01
beta1: 0.9
beta2: 0.999

# Optional Muon-specific fields (all have defaults):
muon_beta: 0.95
muon_ns_steps: 5
muon_ns_coeffs: [3.4445, -4.7750, 2.0315]
muon_nesterov: true
muon_adaptive: false


hf_cg_max_iters: 15   # instead of 50
hf_cg_tol: 1e-2      # looser tolerance
hf_damping: 1e-2

# --- PN-S EigenAdam–specific hyperparameters ---
pns_curvature_update_every: 3   # recompute eigenbasis every N steps
pns_max_eigenvectors: 5          # number of eigen-directions to track
pns_lanczos_iters: 5            # Lanczos iterations (None = use max_eigenvectors)
pns_precond_damping: 1.0e-5      # δ in (Λ + δI)^{-1}
pns_curvature_backend: ggn     # or "kronecker" if you implement that

schedule_free: False
schedule_free_lr: 0.01
schedule_free_b1: 0.9
schedule_free_weight_lr_power: 2.0
