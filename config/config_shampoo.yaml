seed: 0
print_progress: false

dataset: "tiny_imagenet"   # or "tiny_imagenet"
batch_size: 64
image_size: 64             # TinyImageNet uses 64; override when switching datasets
num_epochs: 100
num_channels: 3            # TinyImageNet uses 3 channels
num_classes: 200            # TinyImageNet uses 200 classes

#dataset: "fashion_mnist"   # or "tiny_imagenet"
#batch_size: 64
#image_size: 28            
#num_epochs: 10
#num_channels: 1            
#num_classes: 10           

model: "vit_small"  # or "mlp", "vit_small", "small_resnet", "resnet18"

# Vision Transformer defaults (only used if model starts with "vit")
vit_patch_size: 4
vit_hidden_size: 128
vit_mlp_dim: 256
vit_layers: 4
vit_heads: 4
vit_dropout: 0.1

optim: shampoo  # or "muon", "adam", "soap", "pns_eigenadam", "hessian_free", "shampoo"
lr: 0.001
weight_decay: 0.01
beta1: 0.9
beta2: 0.999

# Optional Muon-specific fields (all have defaults):
muon_beta: 0.95
muon_ns_steps: 5
muon_ns_coeffs: [3.4445, -4.7750, 2.0315]
muon_nesterov: true
muon_adaptive: false


hf_cg_max_iters: 15   # instead of 50
hf_cg_tol: 1e-2      # looser tolerance
hf_damping: 1e-2

# --- PN-S EigenAdamâ€“specific hyperparameters ---
pns_curvature_update_every: 24     
pns_max_eigenvectors: 12           
pns_lanczos_iters: 15              
pns_precond_damping: 1.0e-4       
pns_curvature_backend: ggn        

schedule_free: False
schedule_free_lr: 0.01
schedule_free_b1: 0.9
schedule_free_weight_lr_power: 2.0
