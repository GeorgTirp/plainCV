## Local LM config (JAX/Flax)
## Update trainset_path/validset_path to match your prepared dataset outputs.

deterministic: False
seed: 0
force_cpu: False
matmul_precision: "highest"

# Dataset (HuggingFace datasets saved via data/datasets/prepare.py)
# Example output structure:
# data/datasets/outputs/<dataset_name>/tokenized_<tokenizer_name>/ctx_<seq_len>/train
#trainset_path: "data/datasets/outputs/fwedu/fwedu_sample_10BT_tokenizer_GPT2/tokenized_gpt2/ctx_2048/train"
#validset_path: "data/datasets/outputs/fwedu/fwedu_sample_10BT_tokenizer_GPT2/tokenized_gpt2/ctx_2048/valid"
trainset_path: "data/datasets/outputs/wikitext2/tokenized_gpt2/ctx_128/train"
validset_path: "data/datasets/outputs/wikitext2/tokenized_gpt2/ctx_128/valid"
seq_len: 128
vocab_size: 50257


#vocab_size: 50257
#seq_len: 2048
intra_doc_masking: True
sampler: "stateful_random"
sampler_seed: 0
num_workers: 0

eval: True
valid_tokens: 200000
eval_every_steps: 100

model: "transformer"
d_model: 512
mlp_class: "glu"
expand: "8/3"
n_layers: 6
n_heads: 8
rms_norm: True
tie_embeddings: False
rope_theta: 500000.0
torch_compile: False

# Step budget = token_budget / (seq_len * micro_batch_size * grad_accumulation_steps * world_size)
steps_budget: 200
micro_batch_size: 2
grad_accumulation_steps: 1

# choose between {float32, float16, bfloat16}
dtype: "float32"

optim: "muon"
fused_optim: False
lr: 3.e-4
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0

pns_curvature_update_every: 64 
curvature_eigenvectors: 8       
curvature_iters: 8
gradient_eigenvectors: 6              
gradient_iters: 6
pns_precond_damping: 1.0e-4
pns_sqrt_scaling: false       
pns_curvature_backend: ggn  # or "hessian", "ggn", "fisher"
pns_log_curvature: true
pns_split_spaces: false
pns_lr_top: 1e-3
pns_lr_perp: 5e-3
pns_batched: true
pns_eigensolver: block_oi
pns_block_iters: 4

scheduler: "warmup_cosine"
warmup_steps: 0.05
cooldown_steps: null
lr_start: 0.0
lr_end: 1.e-5
lr_end_pct: null

log_every_steps: 10
print_progress: True

use_wandb: False
wandb_project: "llm"
wandb_dir: "./exp/llm/wandb"
wandb_run_name: "lm_fwedu_10BT"
exp_name: "lm_fwedu_10BT"
out_dir: "./exp/llm"
over_write: True

resume: False
resume_step: null
resume_exp_name: null

save_last_checkpoint: False
save_intermediate_checkpoints: False
save_every_steps: null
