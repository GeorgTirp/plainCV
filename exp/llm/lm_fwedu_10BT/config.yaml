beta1: 0.9
beta2: 0.95
cooldown_steps: null
curvature_eigenvectors: 8
curvature_iters: 8
d_model: 512
deterministic: false
dtype: float32
eval: true
eval_every_steps: 100
exp_name: lm_fwedu_10BT
expand: 8/3
force_cpu: false
fused_optim: false
grad_accumulation_steps: 1
grad_clip: 1.0
gradient_eigenvectors: 6
gradient_iters: 6
intra_doc_masking: true
log_every_steps: 10
lr: 0.0003
lr_end: 1.0e-05
lr_end_pct: null
lr_start: 0.0
matmul_precision: highest
micro_batch_size: 2
mlp_class: glu
model: transformer
n_heads: 8
n_layers: 6
num_workers: 0
optim: muon
out_dir: ./exp/llm
over_write: true
pns_batched: true
pns_block_iters: 4
pns_curvature_backend: ggn
pns_curvature_update_every: 64
pns_eigensolver: block_oi
pns_log_curvature: true
pns_lr_perp: 5e-3
pns_lr_top: 1e-3
pns_precond_damping: 0.0001
pns_split_spaces: false
pns_sqrt_scaling: false
print_progress: true
resume: false
resume_exp_name: null
resume_step: null
rms_norm: true
rope_theta: 500000.0
sampler: stateful_random
sampler_seed: 0
save_every_steps: null
save_intermediate_checkpoints: false
save_last_checkpoint: false
scheduler: warmup_cosine
seed: 0
seq_len: 128
steps_budget: 200
tie_embeddings: false
torch_compile: false
trainset_path: data/datasets/outputs/wikitext2/tokenized_gpt2/ctx_128/train
use_wandb: false
valid_tokens: 200000
validset_path: data/datasets/outputs/wikitext2/tokenized_gpt2/ctx_128/valid
vocab_size: 50257
wandb_dir: ./exp/llm/wandb
wandb_project: llm
wandb_run_name: lm_fwedu_10BT
warmup_steps: 0.05
weight_decay: 0.1
