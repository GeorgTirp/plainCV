beta1: 0.9
beta2: 0.95
cooldown_steps: null
d_model: 512
deterministic: false
dtype: float32
eval: true
eval_every_steps: 100
exp_name: lm_local
expand: 8/3
fused_optim: false
grad_accumulation_steps: 1
grad_clip: 1.0
intra_doc_masking: true
log_every_steps: 10
lr: 0.0003
lr_end: 1.0e-05
lr_end_pct: null
lr_start: 0.0
micro_batch_size: 2
mlp_class: glu
model: transformer
n_heads: 8
n_layers: 6
num_workers: 0
optim: adamw
out_dir: ./exp/llm
over_write: true
print_progress: true
resume: false
resume_exp_name: null
resume_step: null
rms_norm: true
rope_theta: 500000.0
sampler: stateful_random
sampler_seed: 0
save_every_steps: null
save_intermediate_checkpoints: false
save_last_checkpoint: false
scheduler: warmup_cosine
seed: 0
seq_len: 2048
steps_budget: 200
tie_embeddings: false
torch_compile: false
trainset_path: data/datasets/outputs/fwedu/fwedu_sample_10B_tokenizer_GPT2/tokenized_gpt2/ctx_2048/train
use_wandb: false
valid_tokens: 200000
validset_path: data/datasets/outputs/fwedu/fwedu_sample_10B_tokenizer_GPT2/tokenized_gpt2/ctx_2048/valid
vocab_size: 50257
wandb_dir: ./exp/llm/wandb
wandb_project: llm
wandb_run_name: lm_local
warmup_steps: 0.05
weight_decay: 0.1
