{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer metric comparison\n",
    "\n",
    "Load all `exp/run_*/*_metrics.csv` files and compare evaluation accuracy across optimizers. Switch to `train_accuracy` in the plotting cells if you prefer training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metrics from: <generator object Path.glob at 0x7d6cf42329b0>\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No metrics CSV files found under exp/run_*/",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m metrics_files = \u001b[38;5;28msorted\u001b[39m(base_dir.glob(\u001b[33m\"\u001b[39m\u001b[33mrun_*/*_metrics.csv\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m metrics_files:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo metrics CSV files found under exp/run_*/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m frames = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m csv_path \u001b[38;5;129;01min\u001b[39;00m metrics_files:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No metrics CSV files found under exp/run_*/"
     ]
    }
   ],
   "source": [
    "base_dir = Path(\"exp\")\n",
    "print(f\"Loading metrics from: {base_dir.glob(\"run_*/\")}\")\n",
    "metrics_files = sorted(base_dir.glob(\"run_*/*_metrics.csv\"))\n",
    "\n",
    "if not metrics_files:\n",
    "    raise FileNotFoundError(\"No metrics CSV files found under exp/run_*/\")\n",
    "\n",
    "frames = []\n",
    "for csv_path in metrics_files:\n",
    "    optimizer_name = csv_path.stem.replace(\"_metrics\", \"\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"optimizer\"] = optimizer_name\n",
    "    df[\"metrics_path\"] = csv_path.as_posix()\n",
    "    frames.append(df)\n",
    "\n",
    "metrics_df = pd.concat(frames, ignore_index=True)\n",
    "metrics_df = metrics_df.sort_values([\"optimizer\", \"iteration\"]).reset_index(drop=True)\n",
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "for optimizer, group in metrics_df.groupby(\"optimizer\"):\n",
    "    sorted_group = group.sort_values(\"iteration\")\n",
    "    ax.plot(\n",
    "        sorted_group[\"iteration\"],\n",
    "        sorted_group[\"eval_accuracy\"],\n",
    "        marker=\"o\",\n",
    "        label=optimizer,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Eval accuracy\")\n",
    "ax.set_title(\"Iteration vs eval accuracy\")\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "ax.legend(title=\"Optimizer\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "for optimizer, group in metrics_df.groupby(\"optimizer\"):\n",
    "    sorted_group = group.sort_values(\"wall_time_sec\")\n",
    "    ax.plot(\n",
    "        sorted_group[\"wall_time_sec\"],\n",
    "        sorted_group[\"eval_accuracy\"],\n",
    "        marker=\"o\",\n",
    "        label=optimizer,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Wall clock time (s)\")\n",
    "ax.set_ylabel(\"Eval accuracy\")\n",
    "ax.set_title(\"Wall clock time vs eval accuracy\")\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "ax.legend(title=\"Optimizer\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
